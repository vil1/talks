# Those 10000 case classes I never wrote 

When validating data with Spark, or read/writing it to Kafka topics, the go-to solution is to write a Scala case class or a Java Bean. But what if you had only 5 developers, 10000+ data structures and only a few months to ship your project? Let me show you how the power of hylomorphisms combined with expressive schemas allowed us to write the code that validates and transforms data from dozens of tables from a hundred of data sources, and ship our project in time and on budget. 

### Videos

[LambdaDays 2018](https://www.youtube.com/watch?v=P9dbMclkD7A)
[Scalar 2018](https://www.youtube.com/watch?v=1TUgSaD6cCo)
[ScalaUA 2018](https://www.youtube.com/watch?v=0KCsrDZSmjM)
